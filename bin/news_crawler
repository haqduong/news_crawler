#! /usr/bin/env ruby
# -*- coding: utf-8 -*-

#--
# NewsCrawler - a website crawler
#
# Copyright (C) 2013 - Hà Quang Dương <contact@haqduong.net>
#
# This file is part of NewsCrawler.
#
# NewsCrawler is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# NewsCrawler is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with NewsCrawler.  If not, see <http://www.gnu.org/licenses/>.
#++

require 'optparse'
require 'news_crawler/config'
require 'news_crawler/nc_logger'

require 'news_crawler/downloader'
require 'news_crawler/link_selector/same_domain_selector'
require 'news_crawler/storage/url_queue'

include NewsCrawler::Storage

options = {}

OptionParser.new do | opts |
  opts.banner = "Usage: news_crawler [options] url"

  opts.on('-c', "--app-conf FILE", "Application configuration file") do | f |
    options[:app_conf] = File.expand_path(f)
    raise Errno::ENOENT unless File.exists? options[:app_conf]
  end

  opts.on('-sds', "--sds-conf FILE", "Same domain selector configuration file") do | f |
    options[:sds_conf] = File.expand_path(f)
    raise Errno::ENOENT unless File.exists? options[:sds_conf]
  end

  opts.on('-c', '--[no-]cleardb', "Clear database") do | cd |
    options[:cleardb] = cd
  end

  opts.on('-d', "--max-depth DEPTH", OptionParser::DecimalInteger,
          'Maximum depth of url to crawl') do  | d |
    options[:max_depth] = d
  end

  opts.on('-t', "--time-out TIME", OptionParser::DecimalInteger,
          "Wait time (in sec) before stop crawl (crawler is\'nt stopped immediately but terminated gracefully)", 
          "If time out isn't specified you can stop crawler by press Ctrl-C") do | t |
    options[:time_out] = t
  end
end.parse!


NewsCrawler::CrawlerConfig.load_application_config(options[:app_conf]) unless options[:app_conf].nil?
NewsCrawler::CrawlerConfig.load_samedomainselector_config(options[:sds_conf]) unless options[:sds_conf].nil?

config = SimpleConfig.for :application
NewsCrawler::Storage::RawData.set_engine(config.db.engine.intern)
NewsCrawler::Storage::URLQueue.set_engine(config.db.engine.intern)

if (options[:cleardb])
  URLQueue.clear
  RawData.clear
end

if ARGV.size > 0
  url = ARGV[0]
  begin
    URLQueue.add(url)
    rescue NewsCrawler::Storage::URLQueue::DuplicateURLError
    NewsCrawler::NCLogger.get_logger.info("URL existed")
  end
end

puts "Starting Downloader"
dwl = NewsCrawler::Downloader.new(false)
dwl.async.run

puts "Starting SDS"
se = NewsCrawler::LinkSelector::SameDomainSelector.new(options[:max_depth] || 1, false)
se.async.run

if options[:time_out]
  sleep(options[:time_out])
else
  stop = false
  while(!stop)
    Signal.trap("INT") do | signo |
      stop = true
    end
  end
end

puts "Stoping SDS"
se.graceful_terminate
se.terminate
puts "SDS stopped"
puts "Stoping Downloader"
dwl.graceful_terminate
dwl.terminate
puts "Downloader stopped"
